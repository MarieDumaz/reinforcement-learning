{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CPE 693A: Reinforcement Learning and Control\n",
    "### Spring 2021\n",
    "### MS Final Project\n",
    "### Marie Dumaz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n",
      "/home/mcd0029/anaconda3/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:516: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/home/mcd0029/anaconda3/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:517: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/home/mcd0029/anaconda3/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:518: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/home/mcd0029/anaconda3/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:519: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/home/mcd0029/anaconda3/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:520: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/home/mcd0029/anaconda3/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
      "/home/mcd0029/anaconda3/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:541: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/home/mcd0029/anaconda3/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:542: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/home/mcd0029/anaconda3/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:543: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/home/mcd0029/anaconda3/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:544: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/home/mcd0029/anaconda3/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:545: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/home/mcd0029/anaconda3/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:550: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    }
   ],
   "source": [
    "'''IMPORTS'''\n",
    "\n",
    "import gym\n",
    "import offworld_gym\n",
    "import pickle\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "import numpy as np\n",
    "\n",
    "import keras\n",
    "from keras.models import Model, Sequential\n",
    "from keras.layers import Dense, Activation, Flatten, Conv2D, Input, MaxPooling2D, Concatenate\n",
    "from keras.optimizers import Adam\n",
    "\n",
    "from rl.agents.dqn import DQNAgent\n",
    "from rl.policy import LinearAnnealedPolicy, EpsGreedyQPolicy\n",
    "from rl.memory import SequentialMemory\n",
    "from rl.processors import Processor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1782]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# create the environment\n",
    "env = gym.make('OffWorldDockerMonolithDiscreteSim-v0')\n",
    "env.seed(1782)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deep-Q Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the DQN model\n",
    "def create_network():\n",
    "    \n",
    "    # Create input\n",
    "    img_input = Input(shape=(240, 320, 4), name='img_input')\n",
    "\n",
    "    x = img_input\n",
    "    \n",
    "    # Convolution to extract features from image\n",
    "    x = Conv2D(filters = 4, kernel_size = 5, strides = 2)(x)\n",
    "    x = MaxPooling2D((2, 2))(x)\n",
    "    x = Conv2D(filters = 4, kernel_size = 5, strides = 2)(x)\n",
    "    x = MaxPooling2D((2, 2))(x)\n",
    "    x = Conv2D(filters = 1, kernel_size = 5, strides = 1)(x)\n",
    "    x = Activation('relu')(x)\n",
    "    \n",
    "    # Feedforward NN\n",
    "    x = Flatten()(x)\n",
    "    x = Dense(128)(x)\n",
    "    x = Activation('relu')(x)\n",
    "    x = Dense(256)(x)\n",
    "    x = Activation('relu')(x)\n",
    "\n",
    "    # Output is which action to take\n",
    "    output = Dense(nb_actions)(x)\n",
    "    model = Model(inputs=[img_input], outputs=output)\n",
    "    print(model.summary())\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Class to process observations\n",
    "class RosbotProcessor(Processor):\n",
    "\n",
    "    # Returns observation\n",
    "    def process_observation(self, observation):\n",
    "        return observation\n",
    "\n",
    "    # Batch process \n",
    "    # Returns list of corresponding observation for each state in batch\n",
    "    def process_state_batch(self, batch):\n",
    "        imgs_batch = []\n",
    "        for exp in batch:\n",
    "            imgs = []\n",
    "            configs = []\n",
    "            for state in exp:\n",
    "                imgs.append(np.expand_dims(state[0], 0))\n",
    "                configs.append(np.expand_dims(100, 0))\n",
    "            imgs_batch.append(np.concatenate(imgs, -1))\n",
    "        imgs_batch = np.concatenate(imgs_batch, 0)\n",
    "\n",
    "        return imgs_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "nb_actions = env.action_space.n\n",
    "\n",
    "# Parameters\n",
    "memory_size = 25000\n",
    "window_length = 4\n",
    "total_nb_steps = 50000\n",
    "exploration_anneal_nb_steps = 20000\n",
    "max_eps = 0.8 # Max epsilon\n",
    "min_eps = 0.1 # Minimum epsilon\n",
    "learning_warmup_nb_steps = 50\n",
    "target_model_update = 1e-2\n",
    "learning_rate = 1e-3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DQN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/mcd0029/anaconda3/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:517: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/mcd0029/anaconda3/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:74: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/mcd0029/anaconda3/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:4138: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/mcd0029/anaconda3/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:3976: The name tf.nn.max_pool is deprecated. Please use tf.nn.max_pool2d instead.\n",
      "\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "img_input (InputLayer)       (None, 240, 320, 4)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_1 (Conv2D)            (None, 118, 158, 4)       404       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2 (None, 59, 79, 4)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_2 (Conv2D)            (None, 28, 38, 4)         404       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_2 (MaxPooling2 (None, 14, 19, 4)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_3 (Conv2D)            (None, 10, 15, 1)         101       \n",
      "_________________________________________________________________\n",
      "activation_1 (Activation)    (None, 10, 15, 1)         0         \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 150)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 128)               19328     \n",
      "_________________________________________________________________\n",
      "activation_2 (Activation)    (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 256)               33024     \n",
      "_________________________________________________________________\n",
      "activation_3 (Activation)    (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 4)                 1028      \n",
      "=================================================================\n",
      "Total params: 54,289\n",
      "Trainable params: 54,289\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# Initiliaze observation processor\n",
    "processor = RosbotProcessor()\n",
    "# Initialize policy\n",
    "policy = LinearAnnealedPolicy(EpsGreedyQPolicy(), 'eps', max_eps, min_eps, 0.0, exploration_anneal_nb_steps)\n",
    "\n",
    "# Initialize model\n",
    "model = create_network()\n",
    "# Initialize memory\n",
    "memory = SequentialMemory(limit = memory_size, window_length = window_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create DQN agent\n",
    "dqn = DQNAgent(processor = processor, model = model, nb_actions = nb_actions, memory = memory, nb_steps_warmup = learning_warmup_nb_steps,\n",
    "                   enable_double_dqn = False, target_model_update = target_model_update, policy = policy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/mcd0029/anaconda3/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:174: The name tf.get_default_session is deprecated. Please use tf.compat.v1.get_default_session instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/mcd0029/anaconda3/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:181: The name tf.ConfigProto is deprecated. Please use tf.compat.v1.ConfigProto instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/mcd0029/anaconda3/lib/python3.7/site-packages/keras/optimizers.py:790: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Compile\n",
    "dqn.compile(Adam(lr = learning_rate), metrics = ['mae'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training for 50000 steps ...\n",
      "Interval 1 (0 steps performed)\n",
      "10000/10000 [==============================] - 8014s 801ms/step - reward: 0.0048\n",
      "206 episodes - episode_reward: 0.233 [0.000, 1.000] - loss: 0.006 - mean_absolute_error: 0.609 - mean_q: 0.846 - mean_eps: 0.624 - real_time_factor_for_move: 12.367\n",
      "\n",
      "Interval 2 (10000 steps performed)\n",
      "10000/10000 [==============================] - 8095s 809ms/step - reward: 0.0043\n",
      "175 episodes - episode_reward: 0.246 [0.000, 1.000] - loss: 0.007 - mean_absolute_error: 0.924 - mean_q: 1.262 - mean_eps: 0.275 - real_time_factor_for_move: 12.112\n",
      "\n",
      "Interval 3 (20000 steps performed)\n",
      "10000/10000 [==============================] - 8142s 814ms/step - reward: 0.0055\n",
      "179 episodes - episode_reward: 0.307 [0.000, 1.000] - loss: 0.005 - mean_absolute_error: 0.783 - mean_q: 1.062 - mean_eps: 0.100 - real_time_factor_for_move: 12.075\n",
      "\n",
      "Interval 4 (30000 steps performed)\n",
      "10000/10000 [==============================] - 8174s 817ms/step - reward: 0.0061\n",
      "180 episodes - episode_reward: 0.339 [0.000, 1.000] - loss: 0.002 - mean_absolute_error: 0.524 - mean_q: 0.707 - mean_eps: 0.100 - real_time_factor_for_move: 12.203\n",
      "\n",
      "Interval 5 (40000 steps performed)\n",
      "10000/10000 [==============================] - 8255s 826ms/step - reward: 0.0083\n",
      "done, took 40680.453 seconds\n"
     ]
    }
   ],
   "source": [
    "# Train\n",
    "dqn_train = dqn.fit(env, action_repetition=1, nb_steps=total_nb_steps, visualize=False,  verbose = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save final weights\n",
    "dqn.save_weights('dqn_weigths', overwrite = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Double DQN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "img_input (InputLayer)       (None, 240, 320, 4)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_4 (Conv2D)            (None, 118, 158, 4)       404       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_3 (MaxPooling2 (None, 59, 79, 4)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_5 (Conv2D)            (None, 28, 38, 4)         404       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_4 (MaxPooling2 (None, 14, 19, 4)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_6 (Conv2D)            (None, 10, 15, 1)         101       \n",
      "_________________________________________________________________\n",
      "activation_4 (Activation)    (None, 10, 15, 1)         0         \n",
      "_________________________________________________________________\n",
      "flatten_2 (Flatten)          (None, 150)               0         \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 128)               19328     \n",
      "_________________________________________________________________\n",
      "activation_5 (Activation)    (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 256)               33024     \n",
      "_________________________________________________________________\n",
      "activation_6 (Activation)    (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_6 (Dense)              (None, 4)                 1028      \n",
      "=================================================================\n",
      "Total params: 54,289\n",
      "Trainable params: 54,289\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# Initiliaze observation processor\n",
    "processor = RosbotProcessor()\n",
    "# Initialize policy\n",
    "policy = LinearAnnealedPolicy(EpsGreedyQPolicy(), 'eps', max_eps, min_eps, 0.0, exploration_anneal_nb_steps)\n",
    "\n",
    "# Initialize model\n",
    "model = create_network()\n",
    "# Initialize memory\n",
    "memory = SequentialMemory(limit = memory_size, window_length = window_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create agent\n",
    "double_dqn = DQNAgent(processor = processor, model = model, nb_actions = nb_actions, memory = memory, nb_steps_warmup = learning_warmup_nb_steps,\n",
    "                   enable_double_dqn = True, target_model_update = target_model_update, policy = policy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile\n",
    "double_dqn.compile(Adam(lr = learning_rate), metrics=['mae'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training for 50000 steps ...\n",
      "Interval 1 (0 steps performed)\n",
      "10000/10000 [==============================] - 9296s 930ms/step - reward: 0.0046\n",
      "210 episodes - episode_reward: 0.219 [0.000, 1.000] - loss: 0.005 - mean_absolute_error: 0.442 - mean_q: 0.613 - mean_eps: 0.624 - real_time_factor_for_move: 12.257\n",
      "\n",
      "Interval 2 (10000 steps performed)\n",
      "10000/10000 [==============================] - 9352s 935ms/step - reward: 0.0065\n",
      "189 episodes - episode_reward: 0.344 [0.000, 1.000] - loss: 0.003 - mean_absolute_error: 0.520 - mean_q: 0.706 - mean_eps: 0.275 - real_time_factor_for_move: 12.170\n",
      "\n",
      "Interval 3 (20000 steps performed)\n",
      "10000/10000 [==============================] - 9379s 938ms/step - reward: 0.0089\n",
      "203 episodes - episode_reward: 0.438 [0.000, 1.000] - loss: 0.002 - mean_absolute_error: 0.417 - mean_q: 0.566 - mean_eps: 0.100 - real_time_factor_for_move: 12.329\n",
      "\n",
      "Interval 4 (30000 steps performed)\n",
      "10000/10000 [==============================] - 9445s 944ms/step - reward: 0.0084\n",
      "198 episodes - episode_reward: 0.424 [0.000, 1.000] - loss: 0.002 - mean_absolute_error: 0.416 - mean_q: 0.566 - mean_eps: 0.100 - real_time_factor_for_move: 12.213\n",
      "\n",
      "Interval 5 (40000 steps performed)\n",
      "10000/10000 [==============================] - 9482s 948ms/step - reward: 0.0094\n",
      "done, took 46954.232 seconds\n"
     ]
    }
   ],
   "source": [
    "# Train\n",
    "double_dqn_train = double_dqn.fit(env, action_repetition = 1, nb_steps = total_nb_steps, visualize = False,  verbose = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save final weights\n",
    "double_dqn.save_weights('double_dqn_weigths', overwrite=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Duel Double DQN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "img_input (InputLayer)       (None, 240, 320, 4)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_7 (Conv2D)            (None, 118, 158, 4)       404       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_5 (MaxPooling2 (None, 59, 79, 4)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_8 (Conv2D)            (None, 28, 38, 4)         404       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_6 (MaxPooling2 (None, 14, 19, 4)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_9 (Conv2D)            (None, 10, 15, 1)         101       \n",
      "_________________________________________________________________\n",
      "activation_7 (Activation)    (None, 10, 15, 1)         0         \n",
      "_________________________________________________________________\n",
      "flatten_3 (Flatten)          (None, 150)               0         \n",
      "_________________________________________________________________\n",
      "dense_7 (Dense)              (None, 128)               19328     \n",
      "_________________________________________________________________\n",
      "activation_8 (Activation)    (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_8 (Dense)              (None, 256)               33024     \n",
      "_________________________________________________________________\n",
      "activation_9 (Activation)    (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_9 (Dense)              (None, 4)                 1028      \n",
      "=================================================================\n",
      "Total params: 54,289\n",
      "Trainable params: 54,289\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# Initiliaze observation processor\n",
    "processor = RosbotProcessor()\n",
    "# Initialize policy\n",
    "policy = LinearAnnealedPolicy(EpsGreedyQPolicy(), 'eps', max_eps, min_eps, 0.0, exploration_anneal_nb_steps)\n",
    "\n",
    "# Initialize model\n",
    "model = create_network()\n",
    "# Initialize memory\n",
    "memory = SequentialMemory(limit = memory_size, window_length = window_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create agent\n",
    "duel_dqn = DQNAgent(processor = processor, model = model, nb_actions = nb_actions, memory = memory, nb_steps_warmup = learning_warmup_nb_steps,\n",
    "                   enable_double_dqn = True, target_model_update = target_model_update, policy = policy, enable_dueling_network = True, dueling_type = 'avg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile\n",
    "duel_dqn.compile(Adam(lr = learning_rate), metrics = ['mae'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training for 50000 steps ...\n",
      "Interval 1 (0 steps performed)\n",
      "10000/10000 [==============================] - 9317s 932ms/step - reward: 0.0051\n",
      "185 episodes - episode_reward: 0.276 [0.000, 1.000] - loss: 0.002 - mean_absolute_error: 0.466 - mean_q: 0.645 - mean_eps: 0.624 - real_time_factor_for_move: 11.769\n",
      "\n",
      "Interval 2 (10000 steps performed)\n",
      "10000/10000 [==============================] - 9412s 941ms/step - reward: 0.0059\n",
      "181 episodes - episode_reward: 0.326 [0.000, 1.000] - loss: 0.002 - mean_absolute_error: 0.473 - mean_q: 0.643 - mean_eps: 0.275 - real_time_factor_for_move: 11.924\n",
      "\n",
      "Interval 3 (20000 steps performed)\n",
      "10000/10000 [==============================] - 9453s 945ms/step - reward: 0.0073\n",
      "181 episodes - episode_reward: 0.403 [0.000, 1.000] - loss: 0.002 - mean_absolute_error: 0.421 - mean_q: 0.572 - mean_eps: 0.100 - real_time_factor_for_move: 12.141\n",
      "\n",
      "Interval 4 (30000 steps performed)\n",
      "10000/10000 [==============================] - 9456s 946ms/step - reward: 0.0054\n",
      "174 episodes - episode_reward: 0.310 [0.000, 1.000] - loss: 0.002 - mean_absolute_error: 0.407 - mean_q: 0.551 - mean_eps: 0.100 - real_time_factor_for_move: 11.935\n",
      "\n",
      "Interval 5 (40000 steps performed)\n",
      "10000/10000 [==============================] - 9497s 950ms/step - reward: 0.0072\n",
      "done, took 47136.826 seconds\n"
     ]
    }
   ],
   "source": [
    "# Train\n",
    "duel_dqn_train = duel_dqn.fit(env, action_repetition = 1, nb_steps = total_nb_steps, visualize = False,  verbose = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save final weights\n",
    "duel_dqn.save_weights('duel_dqn_weigths', overwrite = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load weights saved after training\n",
    "dqn.load_weights('dqn_weigths')\n",
    "double_dqn.load_weights('double_dqn_weigths')\n",
    "duel_dqn.load_weights('duel_dqn_weigths')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For DQN\n",
      "Testing for 100 episodes ...\n",
      "Episode 1: reward: 0.000, steps: 100\n",
      "Episode 2: reward: 1.000, steps: 68\n",
      "Episode 3: reward: 1.000, steps: 2\n",
      "Episode 4: reward: 0.000, steps: 100\n",
      "Episode 5: reward: 0.000, steps: 100\n",
      "Episode 6: reward: 0.000, steps: 3\n",
      "Episode 7: reward: 1.000, steps: 99\n",
      "Episode 8: reward: 1.000, steps: 21\n",
      "Episode 9: reward: 0.000, steps: 100\n",
      "Episode 10: reward: 0.000, steps: 100\n",
      "Episode 11: reward: 1.000, steps: 82\n",
      "Episode 12: reward: 0.000, steps: 100\n",
      "Episode 13: reward: 1.000, steps: 25\n",
      "Episode 14: reward: 1.000, steps: 59\n",
      "Episode 15: reward: 0.000, steps: 5\n",
      "Episode 16: reward: 0.000, steps: 100\n",
      "Episode 17: reward: 0.000, steps: 100\n",
      "Episode 18: reward: 1.000, steps: 49\n",
      "Episode 19: reward: 0.000, steps: 100\n",
      "Episode 20: reward: 1.000, steps: 2\n",
      "Episode 21: reward: 0.000, steps: 100\n",
      "Episode 22: reward: 1.000, steps: 66\n",
      "Episode 23: reward: 0.000, steps: 100\n",
      "Episode 24: reward: 1.000, steps: 13\n",
      "Episode 25: reward: 0.000, steps: 20\n",
      "Episode 26: reward: 0.000, steps: 100\n",
      "Episode 27: reward: 1.000, steps: 58\n",
      "Episode 28: reward: 0.000, steps: 100\n",
      "Episode 29: reward: 0.000, steps: 8\n",
      "Episode 30: reward: 1.000, steps: 25\n",
      "Episode 31: reward: 0.000, steps: 100\n",
      "Episode 32: reward: 0.000, steps: 7\n",
      "Episode 33: reward: 0.000, steps: 100\n",
      "Episode 34: reward: 1.000, steps: 67\n",
      "Episode 35: reward: 0.000, steps: 8\n",
      "Episode 36: reward: 0.000, steps: 6\n",
      "Episode 37: reward: 0.000, steps: 100\n",
      "Episode 38: reward: 1.000, steps: 78\n",
      "Episode 39: reward: 1.000, steps: 6\n",
      "Episode 40: reward: 1.000, steps: 7\n",
      "Episode 41: reward: 1.000, steps: 41\n",
      "Episode 42: reward: 0.000, steps: 100\n",
      "Episode 43: reward: 1.000, steps: 10\n",
      "Episode 44: reward: 0.000, steps: 3\n",
      "Episode 45: reward: 0.000, steps: 2\n",
      "Episode 46: reward: 0.000, steps: 15\n",
      "Episode 47: reward: 0.000, steps: 10\n",
      "Episode 48: reward: 0.000, steps: 66\n",
      "Episode 49: reward: 1.000, steps: 28\n",
      "Episode 50: reward: 0.000, steps: 7\n",
      "Episode 51: reward: 0.000, steps: 100\n",
      "Episode 52: reward: 0.000, steps: 100\n",
      "Episode 53: reward: 0.000, steps: 100\n",
      "Episode 54: reward: 1.000, steps: 33\n",
      "Episode 55: reward: 1.000, steps: 76\n",
      "Episode 56: reward: 1.000, steps: 33\n",
      "Episode 57: reward: 1.000, steps: 10\n",
      "Episode 58: reward: 1.000, steps: 1\n",
      "Episode 59: reward: 1.000, steps: 26\n",
      "Episode 60: reward: 0.000, steps: 100\n",
      "Episode 61: reward: 0.000, steps: 24\n",
      "Episode 62: reward: 0.000, steps: 5\n",
      "Episode 63: reward: 0.000, steps: 100\n",
      "Episode 64: reward: 0.000, steps: 100\n",
      "Episode 65: reward: 0.000, steps: 18\n",
      "Episode 66: reward: 1.000, steps: 6\n",
      "Episode 67: reward: 1.000, steps: 37\n",
      "Episode 68: reward: 0.000, steps: 100\n",
      "Episode 69: reward: 1.000, steps: 52\n",
      "Episode 70: reward: 1.000, steps: 2\n",
      "Episode 71: reward: 1.000, steps: 25\n",
      "Episode 72: reward: 1.000, steps: 49\n",
      "Episode 73: reward: 1.000, steps: 14\n",
      "Episode 74: reward: 0.000, steps: 17\n",
      "Episode 75: reward: 0.000, steps: 5\n",
      "Episode 76: reward: 1.000, steps: 49\n",
      "Episode 77: reward: 0.000, steps: 39\n",
      "Episode 78: reward: 0.000, steps: 100\n",
      "Episode 79: reward: 1.000, steps: 14\n",
      "Episode 80: reward: 0.000, steps: 100\n",
      "Episode 81: reward: 1.000, steps: 6\n",
      "Episode 82: reward: 0.000, steps: 13\n",
      "Episode 83: reward: 0.000, steps: 13\n",
      "Episode 84: reward: 1.000, steps: 16\n",
      "Episode 85: reward: 0.000, steps: 5\n",
      "Episode 86: reward: 1.000, steps: 27\n",
      "Episode 87: reward: 1.000, steps: 46\n",
      "Episode 88: reward: 0.000, steps: 100\n",
      "Episode 89: reward: 0.000, steps: 100\n",
      "Episode 90: reward: 1.000, steps: 78\n",
      "Episode 91: reward: 0.000, steps: 100\n",
      "Episode 92: reward: 0.000, steps: 100\n",
      "Episode 93: reward: 1.000, steps: 24\n",
      "Episode 94: reward: 1.000, steps: 34\n",
      "Episode 95: reward: 1.000, steps: 41\n",
      "Episode 96: reward: 0.000, steps: 10\n",
      "Episode 97: reward: 0.000, steps: 20\n",
      "Episode 98: reward: 1.000, steps: 70\n",
      "Episode 99: reward: 0.000, steps: 100\n",
      "Episode 100: reward: 0.000, steps: 100\n",
      "For Double DQN\n",
      "Testing for 100 episodes ...\n",
      "Episode 1: reward: 1.000, steps: 41\n",
      "Episode 2: reward: 0.000, steps: 71\n",
      "Episode 3: reward: 0.000, steps: 100\n",
      "Episode 4: reward: 0.000, steps: 12\n",
      "Episode 5: reward: 0.000, steps: 22\n",
      "Episode 6: reward: 0.000, steps: 100\n",
      "Episode 7: reward: 0.000, steps: 24\n",
      "Episode 8: reward: 0.000, steps: 100\n",
      "Episode 9: reward: 1.000, steps: 13\n",
      "Episode 10: reward: 0.000, steps: 100\n",
      "Episode 11: reward: 0.000, steps: 5\n",
      "Episode 12: reward: 0.000, steps: 44\n",
      "Episode 13: reward: 1.000, steps: 1\n",
      "Episode 14: reward: 1.000, steps: 20\n",
      "Episode 15: reward: 0.000, steps: 8\n",
      "Episode 16: reward: 1.000, steps: 19\n",
      "Episode 17: reward: 0.000, steps: 100\n",
      "Episode 18: reward: 0.000, steps: 59\n",
      "Episode 19: reward: 1.000, steps: 5\n",
      "Episode 20: reward: 1.000, steps: 17\n",
      "Episode 21: reward: 1.000, steps: 25\n",
      "Episode 22: reward: 0.000, steps: 3\n",
      "Episode 23: reward: 1.000, steps: 24\n",
      "Episode 24: reward: 1.000, steps: 22\n",
      "Episode 25: reward: 0.000, steps: 58\n",
      "Episode 26: reward: 0.000, steps: 4\n",
      "Episode 27: reward: 1.000, steps: 9\n",
      "Episode 28: reward: 1.000, steps: 7\n",
      "Episode 29: reward: 0.000, steps: 77\n",
      "Episode 30: reward: 0.000, steps: 12\n",
      "Episode 31: reward: 0.000, steps: 40\n",
      "Episode 32: reward: 0.000, steps: 100\n",
      "Episode 33: reward: 0.000, steps: 40\n",
      "Episode 34: reward: 0.000, steps: 100\n",
      "Episode 35: reward: 0.000, steps: 100\n",
      "Episode 36: reward: 1.000, steps: 27\n",
      "Episode 37: reward: 0.000, steps: 37\n",
      "Episode 38: reward: 0.000, steps: 100\n",
      "Episode 39: reward: 0.000, steps: 17\n",
      "Episode 40: reward: 1.000, steps: 10\n",
      "Episode 41: reward: 0.000, steps: 5\n",
      "Episode 42: reward: 0.000, steps: 33\n",
      "Episode 43: reward: 1.000, steps: 27\n",
      "Episode 44: reward: 1.000, steps: 18\n",
      "Episode 45: reward: 0.000, steps: 75\n",
      "Episode 46: reward: 0.000, steps: 100\n",
      "Episode 47: reward: 0.000, steps: 21\n",
      "Episode 48: reward: 1.000, steps: 51\n",
      "Episode 49: reward: 1.000, steps: 2\n",
      "Episode 50: reward: 1.000, steps: 57\n",
      "Episode 51: reward: 0.000, steps: 22\n",
      "Episode 52: reward: 0.000, steps: 35\n",
      "Episode 53: reward: 0.000, steps: 100\n",
      "Episode 54: reward: 1.000, steps: 15\n",
      "Episode 55: reward: 1.000, steps: 1\n",
      "Episode 56: reward: 1.000, steps: 25\n",
      "Episode 57: reward: 0.000, steps: 100\n",
      "Episode 58: reward: 1.000, steps: 52\n",
      "Episode 59: reward: 0.000, steps: 15\n",
      "Episode 60: reward: 1.000, steps: 22\n",
      "Episode 61: reward: 1.000, steps: 18\n",
      "Episode 62: reward: 0.000, steps: 100\n",
      "Episode 63: reward: 0.000, steps: 100\n",
      "Episode 64: reward: 0.000, steps: 9\n",
      "Episode 65: reward: 1.000, steps: 9\n",
      "Episode 66: reward: 0.000, steps: 100\n",
      "Episode 67: reward: 1.000, steps: 14\n",
      "Episode 68: reward: 0.000, steps: 100\n",
      "Episode 69: reward: 0.000, steps: 14\n",
      "Episode 70: reward: 0.000, steps: 94\n",
      "Episode 71: reward: 0.000, steps: 59\n",
      "Episode 72: reward: 0.000, steps: 100\n",
      "Episode 73: reward: 0.000, steps: 24\n",
      "Episode 74: reward: 0.000, steps: 28\n",
      "Episode 75: reward: 0.000, steps: 100\n",
      "Episode 76: reward: 0.000, steps: 100\n",
      "Episode 77: reward: 1.000, steps: 8\n",
      "Episode 78: reward: 1.000, steps: 12\n",
      "Episode 79: reward: 1.000, steps: 12\n",
      "Episode 80: reward: 0.000, steps: 3\n",
      "Episode 81: reward: 0.000, steps: 21\n",
      "Episode 82: reward: 1.000, steps: 5\n",
      "Episode 83: reward: 0.000, steps: 9\n",
      "Episode 84: reward: 0.000, steps: 7\n",
      "Episode 85: reward: 0.000, steps: 100\n",
      "Episode 86: reward: 0.000, steps: 100\n",
      "Episode 87: reward: 1.000, steps: 33\n",
      "Episode 88: reward: 1.000, steps: 18\n",
      "Episode 89: reward: 0.000, steps: 100\n",
      "Episode 90: reward: 1.000, steps: 19\n",
      "Episode 91: reward: 1.000, steps: 27\n",
      "Episode 92: reward: 0.000, steps: 3\n",
      "Episode 93: reward: 0.000, steps: 100\n",
      "Episode 94: reward: 0.000, steps: 9\n",
      "Episode 95: reward: 0.000, steps: 100\n",
      "Episode 96: reward: 0.000, steps: 51\n",
      "Episode 97: reward: 0.000, steps: 100\n",
      "Episode 98: reward: 0.000, steps: 5\n",
      "Episode 99: reward: 1.000, steps: 18\n",
      "Episode 100: reward: 1.000, steps: 13\n",
      "For Duel Double DQN\n",
      "Testing for 100 episodes ...\n",
      "Episode 1: reward: 0.000, steps: 100\n",
      "Episode 2: reward: 1.000, steps: 70\n",
      "Episode 3: reward: 1.000, steps: 12\n",
      "Episode 4: reward: 0.000, steps: 2\n",
      "Episode 5: reward: 0.000, steps: 100\n",
      "Episode 6: reward: 0.000, steps: 100\n",
      "Episode 7: reward: 1.000, steps: 26\n",
      "Episode 8: reward: 0.000, steps: 24\n",
      "Episode 9: reward: 1.000, steps: 30\n",
      "Episode 10: reward: 0.000, steps: 71\n",
      "Episode 11: reward: 0.000, steps: 72\n",
      "Episode 12: reward: 0.000, steps: 100\n",
      "Episode 13: reward: 0.000, steps: 37\n",
      "Episode 14: reward: 0.000, steps: 100\n",
      "Episode 15: reward: 0.000, steps: 20\n",
      "Episode 16: reward: 0.000, steps: 100\n",
      "Episode 17: reward: 0.000, steps: 5\n",
      "Episode 18: reward: 1.000, steps: 5\n",
      "Episode 19: reward: 1.000, steps: 36\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 20: reward: 0.000, steps: 100\n",
      "Episode 21: reward: 0.000, steps: 8\n",
      "Episode 22: reward: 0.000, steps: 7\n",
      "Episode 23: reward: 0.000, steps: 20\n",
      "Episode 24: reward: 1.000, steps: 22\n",
      "Episode 25: reward: 0.000, steps: 28\n",
      "Episode 26: reward: 1.000, steps: 33\n",
      "Episode 27: reward: 0.000, steps: 100\n",
      "Episode 28: reward: 0.000, steps: 100\n",
      "Episode 29: reward: 0.000, steps: 100\n",
      "Episode 30: reward: 0.000, steps: 100\n",
      "Episode 31: reward: 0.000, steps: 44\n",
      "Episode 32: reward: 0.000, steps: 100\n",
      "Episode 33: reward: 0.000, steps: 9\n",
      "Episode 34: reward: 0.000, steps: 5\n",
      "Episode 35: reward: 0.000, steps: 10\n",
      "Episode 36: reward: 0.000, steps: 21\n",
      "Episode 37: reward: 0.000, steps: 43\n",
      "Episode 38: reward: 0.000, steps: 100\n",
      "Episode 39: reward: 1.000, steps: 22\n",
      "Episode 40: reward: 1.000, steps: 98\n",
      "Episode 41: reward: 0.000, steps: 100\n",
      "Episode 42: reward: 0.000, steps: 4\n",
      "Episode 43: reward: 0.000, steps: 100\n",
      "Episode 44: reward: 1.000, steps: 53\n",
      "Episode 45: reward: 0.000, steps: 3\n",
      "Episode 46: reward: 1.000, steps: 11\n",
      "Episode 47: reward: 0.000, steps: 100\n",
      "Episode 48: reward: 1.000, steps: 12\n",
      "Episode 49: reward: 0.000, steps: 20\n",
      "Episode 50: reward: 0.000, steps: 32\n",
      "Episode 51: reward: 0.000, steps: 38\n",
      "Episode 52: reward: 1.000, steps: 25\n",
      "Episode 53: reward: 0.000, steps: 100\n",
      "Episode 54: reward: 0.000, steps: 12\n",
      "Episode 55: reward: 1.000, steps: 28\n",
      "Episode 56: reward: 1.000, steps: 29\n",
      "Episode 57: reward: 0.000, steps: 32\n",
      "Episode 58: reward: 1.000, steps: 18\n",
      "Episode 59: reward: 0.000, steps: 92\n",
      "Episode 60: reward: 0.000, steps: 100\n",
      "Episode 61: reward: 0.000, steps: 28\n",
      "Episode 62: reward: 0.000, steps: 100\n",
      "Episode 63: reward: 1.000, steps: 5\n",
      "Episode 64: reward: 1.000, steps: 9\n",
      "Episode 65: reward: 0.000, steps: 85\n",
      "Episode 66: reward: 1.000, steps: 23\n",
      "Episode 67: reward: 1.000, steps: 22\n",
      "Episode 68: reward: 0.000, steps: 100\n",
      "Episode 69: reward: 1.000, steps: 21\n",
      "Episode 70: reward: 0.000, steps: 100\n",
      "Episode 71: reward: 1.000, steps: 12\n",
      "Episode 72: reward: 0.000, steps: 5\n",
      "Episode 73: reward: 0.000, steps: 100\n",
      "Episode 74: reward: 0.000, steps: 20\n",
      "Episode 75: reward: 1.000, steps: 16\n",
      "Episode 76: reward: 0.000, steps: 100\n",
      "Episode 77: reward: 0.000, steps: 100\n",
      "Episode 78: reward: 1.000, steps: 69\n",
      "Episode 79: reward: 0.000, steps: 72\n",
      "Episode 80: reward: 0.000, steps: 37\n",
      "Episode 81: reward: 1.000, steps: 20\n",
      "Episode 82: reward: 1.000, steps: 4\n",
      "Episode 83: reward: 0.000, steps: 100\n",
      "Episode 84: reward: 1.000, steps: 28\n",
      "Episode 85: reward: 0.000, steps: 100\n",
      "Episode 86: reward: 0.000, steps: 100\n",
      "Episode 87: reward: 0.000, steps: 100\n",
      "Episode 88: reward: 0.000, steps: 58\n",
      "Episode 89: reward: 0.000, steps: 100\n",
      "Episode 90: reward: 0.000, steps: 8\n",
      "Episode 91: reward: 0.000, steps: 100\n",
      "Episode 92: reward: 0.000, steps: 44\n",
      "Episode 93: reward: 1.000, steps: 8\n",
      "Episode 94: reward: 0.000, steps: 100\n",
      "Episode 95: reward: 0.000, steps: 51\n",
      "Episode 96: reward: 1.000, steps: 39\n",
      "Episode 97: reward: 0.000, steps: 100\n",
      "Episode 98: reward: 0.000, steps: 5\n",
      "Episode 99: reward: 0.000, steps: 74\n",
      "Episode 100: reward: 1.000, steps: 14\n"
     ]
    }
   ],
   "source": [
    "# Evaluate how well our model does for 100 episodes\n",
    "visu = False\n",
    "n = 100\n",
    "print(\"For DQN\")\n",
    "dqn_test = dqn.test(env, nb_episodes = n, visualize = visu)\n",
    "print(\"For Double DQN\")\n",
    "double_dqn_test = double_dqn.test(env, nb_episodes = n, visualize = visu)\n",
    "print(\"For Duel Double DQN\")\n",
    "duel_dqn_test = duel_dqn.test(env, nb_episodes = n, visualize = visu)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DQN success percentage: 0.44\n",
      "Double DQN success percentage: 0.37\n",
      "Duel DQN success percentage: 0.31\n"
     ]
    }
   ],
   "source": [
    "# Reward average\n",
    "dqn_success = np.mean(dqn_test.history['episode_reward'])\n",
    "double_dqn_success = np.mean(double_dqn_test.history['episode_reward'])\n",
    "duel_dqn_success = np.mean(duel_dqn_test.history['episode_reward'])\n",
    "print(f'DQN success percentage: {dqn_success}')\n",
    "print(f'Double DQN success percentage: {double_dqn_success}')\n",
    "print(f'Duel DQN success percentage: {duel_dqn_success}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For DQN\n",
      "Average from all episodes: 61.14\n",
      "Average from successful episodes: 33.97826086956522\n",
      "Percentage of quick episodes out of successful episodes: 0.1956521739130435\n"
     ]
    }
   ],
   "source": [
    "# DQN Average number of steps\n",
    "avg_all = np.mean(dqn_test.history['nb_steps'])\n",
    "successes = [dqn_test.history['nb_steps'][i] for i in range(len(dqn_test.history['nb_steps'])) if dqn_test.history['episode_reward'][i] == 1]\n",
    "avg_success = np.mean(successes)\n",
    "quick_episodes = [i for i in dqn_test.history['nb_steps'] if i < 10]\n",
    "print(\"For DQN\")\n",
    "print(f\"Average from all episodes: {avg_all}\")\n",
    "print(f\"Average from successful episodes: {avg_success}\")\n",
    "print(f\"Percentage of quick episodes out of successful episodes: {len(quick_episodes) / len(successes)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For Double DQN\n",
      "Average from all episodes: 48.48\n",
      "Average from successful episodes: 29.307692307692307\n",
      "Percentage of quick episodes out of successful episodes: 0.2564102564102564\n"
     ]
    }
   ],
   "source": [
    "# Double DQN Average number of steps\n",
    "avg_all = np.mean(double_dqn_test.history['nb_steps'])\n",
    "successes = [double_dqn_test.history['nb_steps'][i] for i in range(len(double_dqn_test.history['nb_steps'])) if double_dqn_test.history['episode_reward'][i] == 1]\n",
    "avg_success = np.mean(successes)\n",
    "quick_episodes = [i for i in double_dqn_test.history['nb_steps'] if i < 10]\n",
    "print(\"For Double DQN\")\n",
    "print(f\"Average from all episodes: {avg_all}\")\n",
    "print(f\"Average from successful episodes: {avg_success}\")\n",
    "print(f\"Percentage of quick episodes out of successful episodes: {len(quick_episodes) / len(successes)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For Duel Double DQN\n",
      "Average from all episodes: 54.47\n",
      "Average from successful episodes: 23.708333333333332\n",
      "Percentage of quick episodes out of successful episodes: 0.6666666666666666\n"
     ]
    }
   ],
   "source": [
    "# Duel DQN Average number of steps\n",
    "avg_all = np.mean(duel_dqn_test.history['nb_steps'])\n",
    "successes = [duel_dqn_test.history['nb_steps'][i] for i in range(len(duel_dqn_test.history['nb_steps'])) if duel_dqn_test.history['episode_reward'][i] == 1]\n",
    "avg_success = np.mean(successes)\n",
    "quick_episodes = [i for i in duel_dqn_test.history['nb_steps'] if i < 10]\n",
    "print(\"For Duel Double DQN\")\n",
    "print(f\"Average from all episodes: {avg_all}\")\n",
    "print(f\"Average from successful episodes: {avg_success}\")\n",
    "print(f\"Percentage of quick episodes out of successful episodes: {len(quick_episodes) / len(successes)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: The callbacks.py script in the rl library was modified to return the metrics so the code below will return an error if run on another computer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extracting metrics\n",
    "# 1st value is the loss, second is the mean absolute error, third is mean q value and fourth is the mean epsilon value\n",
    "dqn_loss = [i for i,j,n,m in dqn_train.history['metrics'] if i == i]\n",
    "dqn_mean_absolute_error = [j for i,j,n,m in dqn_train.history['metrics'] if j == j]\n",
    "dqn_mean_q = [n for i,j,n,m in dqn_train.history['metrics'] if n == n]\n",
    "dqn_mean_eps = [m for i,j,n,m in dqn_train.history['metrics'] if m == m]\n",
    "\n",
    "double_dqn_loss = [i for i,j,n,m in double_dqn_train.history['metrics'] if i == i]\n",
    "double_dqn_mean_absolute_error = [j for i,j,n,m in double_dqn_train.history['metrics'] if j == j]\n",
    "double_dqn_mean_q = [n for i,j,n,m in double_dqn_train.history['metrics'] if n == n]\n",
    "double_dqn_mean_eps = [m for i,j,n,m in double_dqn_train.history['metrics'] if m == m]\n",
    "\n",
    "duel_dqn_loss = [i for i,j,n,m in duel_dqn_train.history['metrics'] if i == i]\n",
    "duel_dqn_mean_absolute_error = [j for i,j,n,m in duel_dqn_train.history['metrics'] if j == j]\n",
    "duel_dqn_mean_q = [n for i,j,n,m in duel_dqn_train.history['metrics'] if n == n]\n",
    "duel_dqn_mean_eps = [m for i,j,n,m in duel_dqn_train.history['metrics'] if m == m]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot Loss variation during training\n",
    "plt.plot(dqn_loss, label = \"DQN\")\n",
    "plt.plot(double_dqn_loss, label = \"Double DQN\")\n",
    "plt.plot(duel_dqn_loss, label = \"Duel DQN\")\n",
    "plt.legend()\n",
    "plt.title(\"Loss comparison\", fontsize = 20)\n",
    "plt.xlabel(\"Episode\", fontsize = 15)\n",
    "plt.ylabel(\"Loss\", fontsize = 15)\n",
    "#plt.savefig(\"/home/mcd0029/Documents/CPE693/loss_compare.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot mean absolute error during training\n",
    "plt.plot(dqn_mean_absolute_error, label = \"DQN\")\n",
    "plt.plot(double_dqn_mean_absolute_error, label = \"Double DQN\")\n",
    "plt.plot(duel_dqn_mean_absolute_error, label = \"Duel DQN\")\n",
    "plt.legend()\n",
    "plt.title(\"Mean Absolute Error comparison\", fontsize = 15)\n",
    "plt.xlabel(\"Episode\", fontsize = 12)\n",
    "plt.ylabel(\"Mean Absolute Error\", fontsize = 12)\n",
    "#plt.savefig(\"/home/mcd0029/Documents/CPE693/mean_error_compare.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Duel DQN independent results\n",
    "duel_dqn_test = duel_dqn.test(env, nb_episodes = 100, visualize = True)\n",
    "duel_dqn_success = np.mean(duel_dqn_test.history['episode_reward'])\n",
    "print(f'Duel DQN success percentage: {duel_dqn_success}')\n",
    "\n",
    "# Duel DQN Average number of steps\n",
    "avg_all = np.mean(duel_dqn_test.history['nb_steps'])\n",
    "successes = [duel_dqn_test.history['nb_steps'][i] for i in range(len(duel_dqn_test.history['nb_steps'])) if duel_dqn_test.history['episode_reward'][i] == 1]\n",
    "avg_success = np.mean(successes)\n",
    "quick_episodes = [i for i in duel_dqn_test.history['nb_steps'] if i < 10]\n",
    "print(\"For Duel Double DQN\")\n",
    "print(f\"Average from all episodes: {avg_all}\")\n",
    "print(f\"Average from successful episodes: {avg_success}\")\n",
    "print(f\"Percentage of quick episodes out of successful episodes: {len(quick_episodes) / len(successes)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
